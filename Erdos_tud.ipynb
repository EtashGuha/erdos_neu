{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring on hard instances full trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"#imports\n",
    "#imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear\n",
    "\n",
    "\n",
    "#from kernel.datasets import get_dataset\n",
    "from itertools import product\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "\n",
    "\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch.distributions import categorical\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "import torch.nn\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import pygraphviz as pgv\n",
    "\n",
    "from torch_geometric.utils import convert as cnv\n",
    "from torch_geometric.utils import sparse as sp\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "#import pygraphviz as pgv\n",
    "\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "\n",
    "\n",
    "from torch.distributions import relaxed_categorical\n",
    "\n",
    "import myfuncs\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from torch_geometric.nn import GINConv, GATConv, global_mean_pool, NNConv, GCNConv\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "\n",
    "\n",
    "from torch_geometric.data import Batch \n",
    "\n",
    "\n",
    "from torch_scatter import scatter_min, scatter_max, scatter_add, scatter_mean\n",
    "\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "\n",
    "\n",
    "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\n",
    "\n",
    "\n",
    "from torch_geometric.utils import dropout_adj, to_undirected, to_networkx\n",
    "from torch_geometric.utils import is_undirected\n",
    "\n",
    "from cut_utils import get_diracs\n",
    "\n",
    "import scipy\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from torch_geometric.nn import SplineConv, global_mean_pool, DataParallel\n",
    "\n",
    "from torch_geometric.data import DataListLoader\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "from networkx.algorithms import graph_clique_number\n",
    "\n",
    "from networkx.algorithms import find_cliques\n",
    "\n",
    "from torch_geometric.nn.norm import graph_size_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visdom \n",
    "from visdom import Visdom \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_grad_flow( named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            \n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "\n",
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom(port=8097)\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if title_name not in self.plots:\n",
    "            self.plots[title_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[title_name], name=split_name, update = 'append')\n",
    "    \n",
    "\n",
    "    def histog(self,title_name,vals):\n",
    "        if title_name not in self.plots:\n",
    "            self.plots[title_name] = self.viz.histogram(X=vals,env=self.env,opts=dict(title=title_name,numbins=20))\n",
    "        else:\n",
    "            self.viz.histogram(X=vals,env=self.env,win=self.plots[title_name],opts=dict(title=title_name,numbins=20, update = 'replace'))\n",
    "  \n",
    "    def gradflow(self, model, title_name):\n",
    "                    title_name = \"Gradflow\"\n",
    "                    layers = []\n",
    "                    ave_grads = []\n",
    "                    for n, p in net.named_parameters():\n",
    "                            if(p.requires_grad) and (\"bias\" not in n):\n",
    "                                     layers.append(str(n))\n",
    "                                     ave_grads.append(p.grad.abs().mean().cpu().numpy())\n",
    "                    if title_name not in self.plots:\n",
    "                        self.plots[title_name]= self.viz.line(X= list(range(len(layers))),Y= np.array(ave_grads),  env = self.env,  opts=dict(fillarea=True, title = title_name, xtick = True,\n",
    "        xtickmin=0, xtickmax=len(layers), xtickvals = list(range(len(layers))), xtickstep=1/len(layers), xticklabels = layers) )    \n",
    "                    else:\n",
    "                        self.viz.line(X= list(range(len(layers))), Y=ave_grads, env = self.env ,win=self.plots[title_name], opts=dict(fillarea=True, title = title_name, xtick = True,\n",
    "        xtickmin=0, xtickmax=len(layers), xtickvals = list(range(len(layers))), xtickstep=1/len(layers), xticklabels = layers, update=\"append\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATAConv(MessagePassing):\n",
    "    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\n",
    "    <https://arxiv.org/abs/1710.10903>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n",
    "        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\n",
    "\n",
    "    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\n",
    "\n",
    "    .. math::\n",
    "        \\alpha_{i,j} =\n",
    "        \\frac{\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n",
    "        \\right)\\right)}\n",
    "        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n",
    "        \\right)\\right)}.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        heads (int, optional): Number of multi-head-attentions.\n",
    "            (default: :obj:`1`)\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
    "            attentions are averaged instead of concatenated.\n",
    "            (default: :obj:`True`)\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope. (default: :obj:`0.2`)\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to a stochastically\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True,\n",
    "                 negative_slope=0.2, dropout=0, bias=True, **kwargs):\n",
    "        super(GATAConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.weight = Parameter(\n",
    "            torch.Tensor(in_channels, heads * out_channels))\n",
    "        self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, size=None):\n",
    "        \"\"\"\"\"\"\n",
    "        if size is None and torch.is_tensor(x):\n",
    "            edge_index, _ = remove_self_loops(edge_index)\n",
    "            edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        if torch.is_tensor(x):\n",
    "            x = torch.matmul(x, self.weight)\n",
    "        else:\n",
    "            x = (None if x[0] is None else torch.matmul(x[0], self.weight),\n",
    "                 None if x[1] is None else torch.matmul(x[1], self.weight))\n",
    "\n",
    "        return self.propagate(edge_index, size=size, x=x)\n",
    "\n",
    "\n",
    "    def message(self, edge_index_i, x_i, x_j, size_i):\n",
    "        # Compute attention coefficients.\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "        if x_i is None:\n",
    "            alpha = (x_j * self.att[:, :, self.out_channels:]).sum(dim=-1)\n",
    "        else:\n",
    "            x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "            alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n",
    "\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index_i, size_i)\n",
    "\n",
    "        # Sample attention coefficients stochastically.\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.concat is True:\n",
    "            aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            aggr_out = aggr_out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv2(MessagePassing):\n",
    "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classfication with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
    "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}`.\n",
    "            (default: :obj:`False`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 improved=False,\n",
    "                 cached=False,\n",
    "                 bias=True):\n",
    "        super(GCNConv2, self).__init__('add')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ),\n",
    "                                     dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "        edge_weight = edge_weight.view(-1)\n",
    "        assert edge_weight.size(0) == edge_index.size(1)\n",
    "\n",
    "#         print(\"before: \")\n",
    "#         print(edge_weight.shape)\n",
    "#         print(edge_index.shape)\n",
    "        \n",
    "        #edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n",
    "        #edge_index = add_self_loops(edge_index, num_nodes)\n",
    "#         loop_weight = torch.full((num_nodes, ),\n",
    "#                                  1 if not improved else 2,\n",
    "#                                  dtype=edge_weight.dtype,\n",
    "#                                  device=edge_weight.device)\n",
    "        #edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "#         print(\"after: \")\n",
    "#         print(edge_weight.shape)\n",
    "#         print(edge_index.shape)\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
    "                                         self.improved, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def propagate(x, edge_index):\n",
    "        row, col = edge_index\n",
    "        out = scatter_add(x[col], row, dim=0)\n",
    "        return out\n",
    "\n",
    "    def get_mask(x, edge_index, hops):\n",
    "        for k in range(hops):\n",
    "            x = propagate(x, edge_index)\n",
    "        mask = (x>0).float()\n",
    "        return mask\n",
    "\n",
    "    \n",
    "    def total_var(x, edge_index, batch, undirected = True):\n",
    "        row, col = edge_index\n",
    "        if undirected:\n",
    "            tv = (torch.abs(x[row]-x[col])) * 0.5\n",
    "        else:\n",
    "            tv = (torch.abs(x[row]-x[col]))\n",
    "        \n",
    "        tv = scatter_add(tv, batch[row], dim=0)\n",
    "        return  tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_memlab import profile, mem_reporter\n",
    "#@profile\n",
    "class cliqueMPNN_hindsight_earlyGAT(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden1, hidden2, deltas, elasticity=0.01, num_iterations = 30):\n",
    "        super(cliqueMPNN_hindsight_earlyGAT, self).__init__()\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "        self.momentum = 0.1\n",
    "        #self.nns = Sequential(\n",
    "                #Linear(2*hidden, hidden*hidden),\n",
    "                #LeakyReLU(0.1))\n",
    "        self.num_iterations = num_iterations\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.deltas = deltas\n",
    "        self.numlayers = num_layers\n",
    "        self.elasticity = elasticity\n",
    "        self.heads = 8\n",
    "        self.concat = True\n",
    "        \n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            self.bns.append(BN(self.heads*self.hidden1, momentum=self.momentum))\n",
    "\n",
    "        \n",
    "#         self.nns = torch.nn.ModuleList()\n",
    "#         for i in range(num_layers-1):\n",
    "#             self.nns.append(Sequential(\n",
    "#                 Linear(2*hidden, hidden*hidden),\n",
    "#                 LeakyReLU(0.1)))\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()        \n",
    "        for i in range(num_layers - 1):\n",
    "                #self.convs.append(GATAConv(self.heads*self.hidden1, self.hidden1, concat=self.concat ,heads=self.heads))\n",
    "                self.convs.append(GINConv(Sequential(\n",
    "            Linear( self.heads*self.hidden1,  self.heads*self.hidden1),\n",
    "            ReLU(),\n",
    "            Linear( self.heads*self.hidden1,  self.heads*self.hidden1),\n",
    "            ReLU(),\n",
    "            BN(self.heads*self.hidden1, momentum=self.momentum),\n",
    "        ),train_eps=True))\n",
    "                #self.convs.append(GCNConv2(self.heads*self.hidden1, self.heads*self.hidden1))\n",
    "        self.bn1 = BN(self.heads*self.hidden1)\n",
    "        #self.conv1 = GATAConv(self.hidden2, self.hidden1, concat=self.concat ,heads=self.heads)\n",
    "       \n",
    "        self.conv1 = GINConv(Sequential(Linear(self.hidden2,  self.heads*self.hidden1),\n",
    "            ReLU(),\n",
    "            Linear( self.heads*self.hidden1,  self.heads*self.hidden1),\n",
    "            ReLU(),\n",
    "            BN(self.heads*self.hidden1, momentum=self.momentum),\n",
    "        ),train_eps=True)\n",
    "\n",
    "        if self.concat:\n",
    "            self.lin1 = Linear(self.heads*self.hidden1, self.hidden1)\n",
    "        else:\n",
    "            self.lin1 = Linear(self.hidden1, self.hidden1)\n",
    "\n",
    "        #self.bn2 = BN(self.hidden1, momentum=self.momentum)\n",
    "        self.lin2 = Linear(self.hidden1, 1)\n",
    "        self.gnorm = graph_size_norm.GraphSizeNorm()\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        #self.conv2.reset_parameters()\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            \n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "            \n",
    "        self.bn1.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        #self.bn2.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, data, edge_dropout = None, penalty_coefficient = 0.25):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "        num_graphs = batch.max().item() + 1\n",
    "        row, col = edge_index     \n",
    "        total_num_edges = edge_index.shape[1]\n",
    "        N_size = x.shape[0]\n",
    "\n",
    "        \n",
    "        if edge_dropout is not None:\n",
    "            edge_index = dropout_adj(edge_index, edge_attr = (torch.ones(edge_index.shape[1], device=device)).long(), p = edge_dropout, force_undirected=True)[0]\n",
    "            edge_index = add_remaining_self_loops(edge_index, num_nodes = batch.shape[0])[0]\n",
    "                \n",
    "        reduced_num_edges = edge_index.shape[1]\n",
    "        current_edge_percentage = (reduced_num_edges/total_num_edges)\n",
    "        no_loop_index,_ = remove_self_loops(edge_index)  \n",
    "        no_loop_row, no_loop_col = no_loop_index\n",
    "\n",
    "        xinit= x.clone()\n",
    "\n",
    "        #x = torch.cat([x.unsqueeze(-1),torch.rand((x.shape[0],self.hidden2-1),device=device)], dim=1)\n",
    "        #x = torch.rand((x.shape[0],self.hidden2),device=device)\n",
    "        #\n",
    "        x = x.unsqueeze(-1)\n",
    "        #mask = get_mask(x,edge_index,1).to(x.dtype)\n",
    "        \n",
    "#         print(\"xbefore:\",x.shape)\n",
    "\n",
    "        mask = get_mask(x,edge_index,1).to(x.dtype)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))# +x\n",
    "        x = x*mask\n",
    "\n",
    "#         print(\"xshape:\",x.shape)\n",
    "#         print(\"mask: \", mask.shape)\n",
    "        \n",
    "        #x = F.leaky_relu(self.conv1(x, edge_index)) + x\n",
    "        #x = self.conv1(x, edge_index)\n",
    "        #print(x.shape)\n",
    "        x = self.gnorm(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "\n",
    "        \n",
    "        #print(x.shape)\n",
    "            \n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            if(x.dim()>1):\n",
    "                x =  x+F.leaky_relu(conv(x, edge_index))\n",
    "                mask = get_mask(mask,edge_index,1).to(x.dtype)\n",
    "                x = x*mask\n",
    "                x = self.gnorm(x)\n",
    "                x = bn(x)\n",
    "\n",
    "\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         mask = get_mask(mask,edge_index,1).to(x.dtype)\n",
    "#         x = x*mask\n",
    "        xpostconvs = x.detach()\n",
    "        #\n",
    "        x = F.leaky_relu(self.lin1(x)) \n",
    "        x = x*mask\n",
    "        #x = self.gnorm(x)\n",
    "        #x = self.bn2(x)\n",
    "\n",
    "\n",
    "        xpostlin1 = x.detach()\n",
    "        #x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = F.leaky_relu(self.lin2(x)) \n",
    "       # x = self.gnorm(x)\n",
    "\n",
    "        x = x*mask\n",
    "\n",
    "\n",
    "        #xprethresh = x.detach()\n",
    "\n",
    "        #x = x/scatter_add(torch.abs(x), batch,0)[batch]\n",
    "\n",
    "        \n",
    "        #calculate min and max\n",
    "        batch_max = scatter_max(x, batch, 0, dim_size= N_size)[0]\n",
    "        batch_max = torch.index_select(batch_max, 0, batch)        \n",
    "        batch_min = scatter_min(x, batch, 0, dim_size= N_size)[0]\n",
    "        batch_min = torch.index_select(batch_min, 0, batch)\n",
    "\n",
    "                \n",
    "        x = (x-batch_min)/(batch_max+1e-6-batch_min)\n",
    "        #x = x*mask + mask*1e-6\n",
    "        \n",
    "\n",
    "        probs=x\n",
    "        \n",
    "        #print(probs.shape)\n",
    "        \n",
    "        x2 = x.detach()              \n",
    "        deg = degree(row).unsqueeze(-1) \n",
    "        totalvol = scatter_add(deg.detach()*torch.ones_like(x, device=device), batch, 0)+1e-6\n",
    "        totalcard = scatter_add(torch.ones_like(x, device=device), batch, 0)+1e-6\n",
    "        \n",
    "                \n",
    "        #volume within receptive field\n",
    "        #recvol_hard = scatter_add(deg*mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \n",
    "        #reccard_hard = scatter_add(mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \n",
    "        \n",
    "        #assert recvol_hard.mean()/totalvol.mean() <=1, \"Something went wrong! Receptive field is larger than total volume.\"\n",
    "\n",
    "        \n",
    "        \n",
    "        x2 =  ((x2 - torch.rand_like(x, device = device))>0).float()\n",
    "        \n",
    "        vol_1 = scatter_add(probs*deg, batch, 0)+1e-6\n",
    "        card_1 = scatter_add(probs, batch,0)            \n",
    "        #rec_field = scatter_add(mask, batch, 0)+1e-6\n",
    "        set_size = scatter_add(x2, batch, 0)\n",
    "#         tv_hard = total_var(x2, edge_index, batch)\n",
    "        vol_hard = scatter_add(deg*x2, batch, 0, dim_size = batch.max().item()+1)+1e-6 \n",
    "#         conduct_hard = tv_hard/vol_hard\n",
    "            \n",
    "#         rec_field_ratio = set_size/rec_field\n",
    "#         rec_field_volratio = vol_hard/recvol_hard\n",
    "        total_vol_ratio = vol_hard/totalvol\n",
    "        \n",
    "        #volume within receptive field\n",
    "        #recvol_hard = scatter_add(deg*mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \n",
    "        #reccard_hard = scatter_add(mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \n",
    "        \n",
    "        \n",
    "        #calculating the terms for the expected distance between clique and graph\n",
    "        pairwise_prodsums = torch.zeros(num_graphs, device = device)\n",
    "        for graph in range(num_graphs):\n",
    "            batch_graph = (batch==graph)\n",
    "            pairwise_prodsums[graph] = (torch.conv1d(probs[batch_graph].unsqueeze(-1), probs[batch_graph].unsqueeze(-1))).sum()/2\n",
    "        \n",
    "        \n",
    "        ##############CAREFUL\n",
    "        self_sums = scatter_add((probs*probs), batch, 0, dim_size = num_graphs)/2.\n",
    "\n",
    "        #self_sums = scatter_add((probs*probs), batch, 0, dim_size = num_graphs)/1.\n",
    "        \n",
    "        #expected_weight_G = scatter_add(probs[no_loop_row]*probs[no_loop_col], batch[no_loop_row], 0, dim_size = num_graphs)/2\n",
    "        \n",
    "        expected_weight_G = scatter_add(probs[no_loop_row]*probs[no_loop_col], batch[no_loop_row], 0, dim_size = num_graphs)/2.\n",
    "        expected_clique_weight = (pairwise_prodsums.unsqueeze(-1) - self_sums)/1.\n",
    "        expected_distance = (expected_clique_weight - expected_weight_G)\n",
    "        \n",
    "        \n",
    "        #lambda_factors = (torch.rand((30,1), device=device))*penalty_coefficient-0.10\n",
    "        \n",
    "        \n",
    "        #hindsight = torch.ones_like(lambda_factors)*expected_distance.unsqueeze(-1)*0.5 - lambda_factors*expected_weight_G.unsqueeze(-1)\n",
    "        \n",
    "\n",
    "       # print(expected_clique_weight.shape)\n",
    "        #print(self_sums.shape)\n",
    "\n",
    "       # expected_loss =  #torch.median(hindsight, 1)[0]\n",
    "        #print(expected_loss.shape)\n",
    "        \n",
    "        max_set_weight = (scatter_add(torch.ones_like(x)[no_loop_row], batch[no_loop_row], 0, dim_size = num_graphs)/2).squeeze(-1)\n",
    "        \n",
    "        #print(\"how many:\", (data.batch==0).sum())\n",
    "        \n",
    "        set_weight = (scatter_add(x2[no_loop_row]*x2[no_loop_col], batch[no_loop_row], 0, dim_size = num_graphs)/2)+1e-6\n",
    "        clique_edges_hard = (set_size*(set_size-1)/2) +1e-6\n",
    "        clique_dist_hard = set_weight/clique_edges_hard\n",
    "        \n",
    "        \n",
    "#          print(\"cardinalities: \", cardinalities)\n",
    "#         print(\"max_set_weight:\", max_set_weight)\n",
    "#        print(\"max possible weight: \", max_possible_weight)\n",
    "        #print(max_set_weight.shape)\n",
    "        #print(max_possible_weight.shape)\n",
    "        cardinalities =  scatter_add(torch.ones_like(data.batch),data.batch,dim=0).float()\n",
    "        max_possible_weight = (cardinalities*(cardinalities-1)/2)\n",
    "#         diff = (max_possible_weight - max_set_weight) \n",
    "        penalty_coefficient =   (max_set_weight/max_possible_weight) * penalty_coefficient\n",
    "        #penalty_coefficient =  penalty_coefficient*(clique_dist_hard/(1-clique_dist_hard))\n",
    "                \n",
    "        #penalty_coefficient[clique_dist_hard>0.95] = 1.\n",
    "\n",
    "        #print(\"pen coeff:\", penalty_coefficient)\n",
    "#         print(\"Diff: \", diff)\n",
    "#         print(\"penalties:\", penalties.shape)\n",
    "#         print(\"exp dist:\", expected_distance.shape)\n",
    "        \n",
    "        #assert ((max_possible_weight<max_set_weight).sum())<=1e-6, \"Invalid calculation\"\n",
    "        expected_ratio = expected_weight_G/expected_distance\n",
    "        expected_ratio = expected_ratio.detach()\n",
    "        #penalty_coefficient = expected_ratio*penalty_coefficient\n",
    "       # print(expected_ratio.shape)\n",
    "                               \n",
    "        #expected_loss = (penalty_coefficient)*expected_distance*0.5*expected_weight_G - 0.5*expected_weight_G\n",
    "        #expected_loss = expected_clique_weight/expected_weight_G\n",
    "        expected_loss = (penalty_coefficient)*expected_distance*0.5 - 0.5*expected_weight_G\n",
    "        #expected_loss = (penalty_coefficient)*expected_clique_weight*0.5- 1.*expected_weight_G\n",
    "        \n",
    "#         print(\"pen coeff: \", penalty_coefficient)\n",
    "#         print(\"loss: \", expected_loss)\n",
    "    \n",
    "    \n",
    "#         for iter_graph in range(num_graphs):\n",
    "#             if clique_dist_hard[iter_graph]<0.5:\n",
    "#                 print(\"problematic graph exp distance: \", expected_distance[iter_graph])\n",
    "#                 print(\"problematic graph exp cardinality : \", card_1[iter_graph])\n",
    "\n",
    "#                 print(\"problematic graph num nodes: \", totalcard[iter_graph])\n",
    "                \n",
    "        clique_check = ((clique_edges_hard != clique_edges_hard))\n",
    "        \n",
    "        \n",
    "        setedge_check  = ((set_weight != set_weight))\n",
    "        \n",
    "        \n",
    "        \n",
    "        assert ((clique_dist_hard>=1.1).sum())<=1e-6, \"Invalid set vol/clique vol ratio.\"\n",
    "        \n",
    "#        print(expected_loss.shape)\n",
    "        \n",
    "       # normalize = cardinalities.max()/cardinalities\n",
    "        #print(f\"normalize: {normalize}\")\n",
    "        loss = expected_loss#*normalize\n",
    "        #loss = expected_loss*(1/(clique_dist_hard+0.1))\n",
    "\n",
    "\n",
    "        retdict = {}\n",
    "        \n",
    "        retdict[\"output\"] = [probs.squeeze(-1),\"hist\"]   #output\n",
    "        #retdict[\"clique_check\"] = [clique_edges_hard, \"hist\"]\n",
    "        #retdict[\"set_weight_check\"] = [set_weight, \"hist\"]\n",
    "        #retdict[\"|Expected_vol - Target|\"]= [targetcheck.squeeze(-1), \"hist\"] #absolute distance from targetvol\n",
    "        #retdict[\"Expected_volume\"] = [vol_1.mean(),\"sequence\"] #volume\n",
    "        retdict[\"Expected_cardinality\"] = [card_1.mean(),\"sequence\"]\n",
    "        retdict[\"Expected_cardinality_hist\"] = [card_1,\"hist\"]\n",
    "        retdict[\"losses histogram\"] = [loss.squeeze(-1),\"hist\"]\n",
    "        retdict[\"Set sizes\"] = [set_size.squeeze(-1),\"hist\"]\n",
    "        retdict[\"volume_hard\"] = [vol_hard.mean(),\"aux\"] #volume2\n",
    "        retdict[\"cardinality_hard\"] = [set_size[0],\"sequence\"] #volumeq\n",
    "        retdict[\"Expected weight(G)\"]= [expected_weight_G.mean(), \"sequence\"]\n",
    "        retdict[\"Expected maximum weight\"] = [expected_clique_weight.mean(),\"sequence\"]\n",
    "        retdict[\"Expected distance\"]= [expected_distance.mean(), \"sequence\"]\n",
    "        #retdict[\"cut1\"] = [tv.mean(),\"sequence\"] #cut1\n",
    "        #retdict[\"cut_hard\"] = [tv_hard.mean(),\"sequence\"] #cut1\n",
    "        #retdict[\"Average cardinality ratio of receptive field \"] = [rec_field_ratio.mean(),\"sequence\"] \n",
    "        #retdict[\"Recfield volume/Total volume\"] = [recvol_hard.mean()/totalvol.mean(), \"sequence\"]\n",
    "        #retdict[\"Average ratio of receptive field volume\"]= [rec_field_volratio.mean(),'sequence']\n",
    "        retdict[\"Currvol/Cliquevol\"] = [clique_dist_hard.mean(),'sequence']\n",
    "        retdict[\"Currvol/Cliquevol all graphs in batch\"] = [clique_dist_hard.squeeze(-1),'hist']\n",
    "        retdict[\"Average ratio of total volume\"]= [total_vol_ratio.mean(),'sequence']\n",
    "        #retdict[\"penalty coffs\"] = [penalty_coefficient.squeeze(-1),\"hist\"]\n",
    "        retdict[\"cardinalities\"] = [cardinalities.squeeze(-1),\"hist\"]\n",
    "        retdict[\"Current edge percentage\"] = [torch.tensor(current_edge_percentage),'sequence']\n",
    "        #retdict[\"Clique Weight hard\"] = [clique_edges_hard[0], 'sequence']\n",
    "        #retdict[\"Set weight\"] = [set_weight[0], 'sequence']\n",
    "        #retdict[\"mask\"] = [mask, \"aux\"] #mask\n",
    "        #retdict[\"xinit\"] = [xinit,\"hist\"] #layer input diracs\n",
    "        #retdict[\"xpostlin1\"] = [xpostlin1.mean(1),\"hist\"] #after first linear layer\n",
    "        #retdict[\"xprethresh\"] = [xprethresh.mean(1),\"hist\"] #pre thresholding activations 195 x 1\n",
    "        #retdict[\"xsoftbin\"] = [xsoftbin.mean(1),\"hist\"] #soft binarized output\n",
    "        #retdict[\"lossvol\"] = [lossvol.mean(),\"sequence\"] #volume constraint\n",
    "        #retdict[\"losscard\"] = [losscard.mean(),\"sequence\"] #cardinality constraint\n",
    "        retdict[\"loss\"] = [loss.mean().squeeze(),\"sequence\"] #final loss\n",
    "\n",
    "        return retdict\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, recfield):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    avg_cliqdist = 0\n",
    "    exp_cardinalities = torch.tensor(0)\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        #print(\"data  batchsum: \",(data.batch==1).sum())\n",
    "        data = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = recfield)\n",
    "        data = data.to(device)\n",
    "        #print(\"data prime batchsum: \",(data.batch==1).sum())\n",
    "        retdict = model(data)\n",
    "        avg_loss += retdict['loss'][0].item()/len(data_loader)\n",
    "        avg_cliqdist += retdict[\"Currvol/Cliquevol\"][0].item()/len(data_loader)\n",
    "        exp_cardinalities = [retdict[\"Expected_cardinality_hist\"][0]]\n",
    "    #print(\"retdict: \", retdict[\"Expected_cardinality_hist\"][0] )\n",
    "    return avg_loss, avg_cliqdist, exp_cardinalities \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "def solve_gurobi_maxclique(nx_graph, time_limit = None):\n",
    "\n",
    "    nx_complement = nx.operators.complement(nx_graph)\n",
    "    x_vars = {}\n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.params.OutputFlag = 0\n",
    "\n",
    "    if time_limit:\n",
    "        m.params.TimeLimit = time_limit\n",
    "\n",
    "    for node in nx_complement.nodes():\n",
    "        # Create a new model\n",
    "\n",
    "        # Create variables\n",
    "        x_vars['x_'+str(node)] = m.addVar(vtype=GRB.BINARY, name=\"x_\"+str(node))\n",
    "\n",
    "    count_edges = 0\n",
    "    for edge in nx_complement.edges():\n",
    "        m.addConstr(x_vars['x_'+str(edge[0])] + x_vars['x_'+str(edge[1])] <= 1,'c_'+str(count_edges))\n",
    "        count_edges+=1\n",
    "    # Set objective\n",
    "    m.setObjective(sum([x_vars['x_'+str(node)] for node in nx_complement.nodes()]), GRB.MAXIMIZE);\n",
    "\n",
    "\n",
    "    # Optimize model\n",
    "    m.optimize();\n",
    "\n",
    "# for v in m.getVars():\n",
    "#     print('%s %g' % (v.varName, v.x))\n",
    "\n",
    "# print('Obj: %g' % m.objVal)\n",
    "    set_size = m.objVal;\n",
    "    x_vals = [var.x for var in m.getVars()] \n",
    "\n",
    "    return set_size, x_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"TWITTER_SNAP\", \"COLLAB\", \"IMDB-BINARY\"]\n",
    "dataset_name = datasets[0]\n",
    "#datasetname = \"COLLAB_shuffle_1\"\n",
    "#datasetname = \"TWITTER_SNAP\"\n",
    "#dataset_name = \"IMDB-BINARY\"\n",
    "\n",
    "if dataset_name == \"TWITTER_SNAP\":\n",
    "    stored_dataset = open('datasets/TWITTER_SNAP.p', 'rb')        \n",
    "elif dataset_name == \"COLLAB\":\n",
    "    stored_dataset = open('datasets/dataset_shuffle_1'+'.p', 'rb')\n",
    "elif dataset_name == \"IMDB-BINARY\":\n",
    "    stored_dataset = open('datasets/IMDB_BINARY.p', 'rb')\n",
    "#dataset = pickle.load(stored_dataset)\n",
    "\n",
    "\n",
    "dataset = pickle.load(stored_dataset)\n",
    "\n",
    "dataset_scale = 1\n",
    "total_samples = int(np.floor(len(dataset)*dataset_scale))\n",
    "dataset = dataset[:total_samples]\n",
    "\n",
    "num_trainpoints = int(np.floor(0.6*len(dataset)))\n",
    "num_valpoints = int(np.floor(num_trainpoints/3))\n",
    "num_testpoints = len(dataset) - (num_trainpoints + num_valpoints)\n",
    "\n",
    "\n",
    "traindata= dataset[0:num_trainpoints]\n",
    "valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\n",
    "testdata = dataset[num_trainpoints + num_valpoints:]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#set up random seeds \n",
    "torch.manual_seed(1)\n",
    "np.random.seed(2)   \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #traindata_x = [data for data in traindata if data.x.shape[0]>400]\n",
    "# traindata_c = traindata\n",
    "# train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "!cd datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### epochs=1000\n",
    "#number of propagation layers\n",
    "numlayers=5\n",
    "elasticity = 0.2\n",
    "\n",
    "#number of propagation layers + attention\n",
    "receptive_field= numlayers + 1\n",
    "\n",
    "\n",
    "val_losses = []\n",
    "cliq_dists = []\n",
    "\n",
    "net =  cliqueMPNN_hindsight_earlyGAT(dataset,numlayers, 32, 32,1, elasticity = elasticity)\n",
    "#net =  cutTMPNN(dataset,numlayers, 64,1)\n",
    "\n",
    "#envname = \"Hard instance experiments\" + str(net)+ 'PROTOTYPING_' + datasetname + '_seed: '+ str(r_seed) +\"_width1: \"+ str(hidden_1) + \"_batch size : \" + str(batch_size) + \"Finalize_Samples_new_code_GCAT\" + \"_LR: \" +str(learning_rate) + \"_PenaltyCoeff:  \"+ str(penalty_coeff)+ \"_Layers: \"+ str(numlayers) \n",
    "\n",
    "#net =  DataParallel(net)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr_decay_step_size = 5\n",
    "lr_decay_factor = 0.95\n",
    "\n",
    "net.to(device).reset_parameters()\n",
    "#net = net.to(device)\n",
    "\n",
    "optimizer = Adam(net.parameters(), lr=0.001, weight_decay=0.00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MODEL INSTEAD OF TRAINING\n",
    "# net =  cliqueMPNN(dataset,numlayers, 64, 64,1, elasticity = elasticity)\n",
    "\n",
    "# file_name = '/home/karalias/myrepos/CliqueMPNN/cliqueMPNN_randomround_samples/Models/'+ str(datasetname)+ '/' + str(net)+ '_'+ str(75)+'_samples.pt'\n",
    "# cpoint = torch.load(file_name)\n",
    "# net.load_state_dict(cpoint[file_name], strict=False)\n",
    "\n",
    "# net.to(device).reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting up a new session...\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  0\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 14% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  1\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 22% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  2\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 22% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  3\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  7% | 22% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  4\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 22% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  5\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 10% | 22% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  6\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 22% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  7\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 22% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  8\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 12% | 23% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  9\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 24% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  10\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 13% | 24% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  11\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 24% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  12\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 24% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  13\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 21% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  14\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  15\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 24% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  16\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 12% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  17\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  18\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  19\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  20\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  21\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  22\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 22% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  23\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  24\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 14% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  25\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  26\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 23% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  27\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  28\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  29\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 18% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  30\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 13% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  31\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  32\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  33\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  34\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 17% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  35\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 15% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  36\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 10% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  37\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  38\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 15% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  39\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  40\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  41\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 17% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  42\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  43\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  44\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  45\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 12% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  46\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  47\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 14% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  48\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  49\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  50\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  7% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  51\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 10% | 26% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  52\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 12% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  53\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  54\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  55\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  56\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  57\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  8% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  58\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  59\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  60\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  61\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  62\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  5% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  63\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  64\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 18% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  65\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  66\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  67\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 22% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  68\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 17% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  69\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  70\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  71\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 17% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  72\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  73\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 12% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  74\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  75\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  76\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  77\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  78\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  79\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  80\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  81\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 14% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  82\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  5% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  83\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 14% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  84\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  85\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  5% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  86\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  87\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 22% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  88\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  89\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  90\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  91\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  92\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  93\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  94\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 26% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  95\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  7% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  96\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 15% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  97\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 13% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  98\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 27% |\n",
      "|  1 |  0% |  0% |\n",
      "here2\n",
      "Epoch:  99\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 21% | 27% |\n",
      "|  1 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "#THEEEEEEEEEEEEEEEEEESEEEEEEEEEEEEE ONES WWWWWWWWWWWWWWWWOOOOOOOOOOOOORKKKKKKKKKKK\n",
    "# epochs = 200\n",
    "# net.train()\n",
    "# retdict = {}\n",
    "# edge_drop_p = 0.0\n",
    "# edge_dropout_decay = 0.90\n",
    "# penalty_coeff = 9.00\n",
    "# penalty_increase = -0.00\n",
    "# validation_timeout = 500\n",
    "\n",
    "b_sizes = [32]\n",
    "l_rates = [0.001]\n",
    "depths = [4]\n",
    "coefficients = [4.]\n",
    "rand_seeds = [66]\n",
    "widths = [64]\n",
    "\n",
    "epochs = 100\n",
    "net.train()\n",
    "retdict = {}\n",
    "edge_drop_p = 0.0\n",
    "edge_dropout_decay = 0.90\n",
    "penalty_coeff = 9.00\n",
    "penalty_increase = -0.00\n",
    "validation_timeout = 75\n",
    "\n",
    "# b_sizes = [32]\n",
    "# l_rates = [0.001]\n",
    "# depths = [3]\n",
    "# coefficients = [3.5]\n",
    "# rand_seeds = [68]\n",
    "# widths = [64]\n",
    "\n",
    "#THEBEST\n",
    "# epochs = 100\n",
    "# net.train()\n",
    "# retdict = {}\n",
    "# edge_drop_p = 0.0\n",
    "# edge_dropout_decay = 0.90\n",
    "# penalty_coeff = 9.00\n",
    "# penalty_increase = -0.00\n",
    "# validation_timeout = 75\n",
    "\n",
    "\n",
    "\n",
    "for batch_size, learning_rate, numlayers, penalty_coeff, r_seed, hidden_1 in product(b_sizes, l_rates, depths, coefficients, rand_seeds, widths):\n",
    "   \n",
    "    torch.manual_seed(r_seed)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "    val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "    receptive_field= numlayers + 1\n",
    "    val_losses = []\n",
    "    cliq_dists = []\n",
    "\n",
    "    #hidden_1 = 128\n",
    "    hidden_2 = 1\n",
    "\n",
    "    net =  cliqueMPNN_hindsight_earlyGAT(dataset,numlayers, hidden_1, hidden_2 ,1, elasticity = elasticity)\n",
    "    #net =  cutTMPNN(dataset,numlayers, 64,1)\n",
    "\n",
    "    #net =  DataParallel(net)\n",
    "\n",
    "    net.to(device).reset_parameters()\n",
    "    #net = net.to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.00000)\n",
    "\n",
    "    envname = \"August\" + str(net)+ 'PROTOTYPING_' + dataset_name + '_seed: '+ str(r_seed) +\"_width1: \"+ str(hidden_1) + \"_batch size : \" + str(batch_size) + \"Finalize_Samples_new_code_GCAT\" + \"_LR: \" +str(learning_rate) + \"_PenaltyCoeff:  \"+ str(penalty_coeff)+ \"_Layers: \"+ str(numlayers) \n",
    "    \n",
    "    plotter = VisdomLinePlotter(env_name=envname)\n",
    "    for epoch in range(epochs):\n",
    "        totalretdict = {}\n",
    "        count=0\n",
    "        if epoch % 5 == 0:\n",
    "            edge_drop_p = edge_drop_p*edge_dropout_decay\n",
    "            print(\"Edge_dropout: \", edge_drop_p)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            penalty_coeff = penalty_coeff + 0.\n",
    "            print(\"Penalty_coefficient: \", penalty_coeff)\n",
    "\n",
    "        # if epoch % 15 == 0:\n",
    "        #     val_l, cliq_dist, val_cardinalities= predict(net, val_loader, receptive_field)\n",
    "        #     #val_cardinalities = torch.tensor(val_cardinalities.detach().cpu())\n",
    "        #     #print(val_cardinalities)\n",
    "        #     val_losses += [val_l]\n",
    "        #     cliq_dists += [cliq_dist]\n",
    "            # if epoch>30:\n",
    "            #     plotter.plot(\"validation loss\", \"validation loss\", \"validation loss\", epoch, val_losses[-1])\n",
    "            #     plotter.plot(\"validation clique dist\", \"validation clique dist\", \"validation clique dist\", epoch, cliq_dists[-1])\n",
    "            #     #plotter.histog(\"validation expected cardinalities\",val_cardinalities[0])\n",
    "\n",
    "            #     if (val_losses[-1] > val_losses[-2]) and (val_losses[-1] > val_losses[-3]) and (epoch>validation_timeout):\n",
    "            #         print(\"Converged!\")\n",
    "            #         if val_losses[-2] < val_losses[-3]:\n",
    "            #             print(\"Best model epoch: \", epoch-15)\n",
    "            #             file_name = '/home/karalias/myrepos/CliqueMPNN/cliqueMPNN_randomround_samples/Models/'+ str(dataset_name)+ '/' + str(net)+ '_'+ str(epoch-15)+'_samples.pt'\n",
    "            #             net.load_state_dict(torch.load(file_name), strict=False)\n",
    "            #         else:\n",
    "            #             print(\"Best model epoch: \", epoch-30)\n",
    "            #             file_name = '/home/karalias/myrepos/CliqueMPNN/cliqueMPNN_randomround_samples/Models/'+ str(dataset_name)+ '/' + str(net)+ '_'+ str(epoch-30)+'_samples.pt'\n",
    "            #             net.load_state_dict(torch.load(file_name), strict=False)\n",
    "            #         break\n",
    "            #     if epoch%15==0:\n",
    "            #         file_name = '/home/karalias/myrepos/CliqueMPNN/cliqueMPNN_randomround_samples/Models/'+ str(dataset_name)+ '/' + str(net)+ '_'+ str(epoch)+'_samples.pt'\n",
    "            #         print(\"file_name: \", file_name)\n",
    "            #         torch.save({file_name : net.state_dict()},  file_name)\n",
    "\n",
    "        print(\"here2\")\n",
    "        #learning rate schedule\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "        #show currrent epoch and GPU utilizationss\n",
    "        print('Epoch: ', epoch)\n",
    "        GPUtil.showUtilization()\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"here3\")\n",
    "\n",
    "\n",
    "        net.train()\n",
    "        for data in train_loader:\n",
    "            count += 1 \n",
    "            optimizer.zero_grad(), \n",
    "            data = data.to(device)\n",
    "            data_prime = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = receptive_field)\n",
    "\n",
    "            data = data.to('cpu')\n",
    "            data_prime = data_prime.to(device)\n",
    "\n",
    "\n",
    "            retdict = net(data_prime, edge_drop_p, penalty_coeff)\n",
    "            #print(retdict)\n",
    "\n",
    "            for key,val in retdict.items():\n",
    "                if \"sequence\" in val[1]:\n",
    "                    if key in totalretdict:\n",
    "                        totalretdict[key][0] += val[0].item()\n",
    "                    else:\n",
    "                        totalretdict[key] = [val[0].item(),val[1]]\n",
    "\n",
    "                elif count == 1 and \"hist\" in val[1]: \n",
    "                    try:\n",
    "                        plotter.histog(key, val[0].reshape(-1))\n",
    "                    except:    \n",
    "                        print(f\"Remember, you need more than 1 value for a histogram! Key: {key}\", val[0].shape)\n",
    "\n",
    "\n",
    "            #print(\"here4\")\n",
    "\n",
    "            if epoch > 2:\n",
    "#                     print(\"pre backward------------------------\")\n",
    "#                     #reporter.report()\n",
    "#                     print(\"after backward----------------------\")\n",
    "                    retdict[\"loss\"][0].backward()\n",
    "                    #reporter.report()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(net.parameters(),1)\n",
    "                    optimizer.step()\n",
    "                    del(retdict)\n",
    "                    if count==1:\n",
    "                          plotter.gradflow(net, \"Gradflow\")\n",
    "\n",
    "        if epoch > -1:        \n",
    "            for key,val in totalretdict.items():\n",
    "                if \"sequence\" in val[1]:\n",
    "                    val[0] = val[0]/(len(train_loader.dataset)/batch_size)\n",
    "                    plotter.plot(key, key, key, epoch, val[0])\n",
    "\n",
    "            del data_prime"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tbatch_size = batch_size\n",
    "num_data_points = num_testpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diracs_2(data, N , sparse = False, flat = False, replace = True, receptive_field = 7, effective_volume_range = 0.1, max_iterations=20):\n",
    "    device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if not sparse:\n",
    "        graphcount =data.num_nodes #number of graphs in data/batch object\n",
    "        totalnodecount = data.x.shape[1] #number of total nodes for each graph \n",
    "        actualnodecount = 0 #cumulative number of nodes\n",
    "        diracmatrix= torch.zeros((graphcount,totalnodecount,N),device=device) #matrix with dirac pulses\n",
    "\n",
    "\n",
    "        for k in range(graphcount):\n",
    "            graph_nodes = data.mask[k].sum() #number of nodes in the graph\n",
    "            actualnodecount += graph_nodes #might not need this, we'll see\n",
    "            probabilities= torch.ones((graph_nodes.item(),1),device=device)/graph_nodes #uniform probs\n",
    "            node_distribution=OneHotCategorical(probs=probabilities.squeeze())\n",
    "            node_sample= node_distribution.sample(sample_shape=(N,))\n",
    "            node_sample= torch.cat((node_sample,torch.zeros((N,totalnodecount-node_sample.shape[1]),device=device)),-1) #concat zeros to fit dataset shape\n",
    "            diracmatrix[k,:]= torch.transpose(node_sample,dim0=-1,dim1=-2) #add everything to the final matrix\n",
    "            \n",
    "        return diracmatrix\n",
    "    \n",
    "    else:\n",
    "            if not is_undirected(data.edge_index):\n",
    "                data.edge_index = to_undirected(data.edge_index)\n",
    "                \n",
    "            original_batch_index = data.batch\n",
    "            original_edge_index = add_remaining_self_loops(data.edge_index, num_nodes = data.batch.shape[0])[0]\n",
    "            #original_edge_index, _, node_mask = remove_isolated_nodes(original_edge_index)\n",
    "            #batch_index = original_batch_index[node_mask]\n",
    "            batch_index = original_batch_index\n",
    "            \n",
    "            graphcount = data.num_graphs #number of graphs in data/batch object\n",
    "            diracmatrix = torch.zeros(0,device=device)\n",
    "            batch_prime = torch.zeros(0,device=device).long()\n",
    "            locationmatrix = torch.zeros(0,device=device).long()\n",
    "            \n",
    "            r,c = original_edge_index\n",
    "            \n",
    "            \n",
    "            global_offset = 0\n",
    "            all_nodecounts = scatter_(\"add\", torch.ones_like(batch_index,device='cpu'), batch_index)\n",
    "            recfield_vols = torch.zeros(graphcount,device=device)\n",
    "            total_vols = torch.zeros(graphcount,device=device)\n",
    "            \n",
    "\n",
    "            for k in range(graphcount):\n",
    "                #get edges of current graph, remember to subtract offset\n",
    "                graph_nodes = all_nodecounts[k]\n",
    "                if graph_nodes==0:\n",
    "                    print(\"all nodecounts: \", all_nodecounts)\n",
    "                graph_edges = (batch_index[r]==k)\n",
    "                graph_edge_index = original_edge_index[:,graph_edges]-global_offset           \n",
    "                gr, gc = graph_edge_index\n",
    "                \n",
    "#                 print(\"Gr: \", gr)\n",
    "#                 print(\"Graph edge index: \", graph_edge_index)\n",
    "#                 print(\"gr max: \", gr.max())\n",
    "                \n",
    "                \n",
    "                \n",
    "                #get dirac\n",
    "                randInt = np.random.choice(range(graph_nodes), N, replace = replace)\n",
    "                node_sample = torch.zeros(N*graph_nodes,device='cpu')\n",
    "                offs  = torch.arange(N, device=device)*graph_nodes\n",
    "                dirac_locations = (offs + torch.from_numpy(randInt).to(device))\n",
    "                node_sample[dirac_locations] = 1\n",
    "                \n",
    "                                \n",
    "                #calculate receptive field volume and compare to total volume\n",
    "                mask = get_mask(node_sample, graph_edge_index.detach(), receptive_field).float()\n",
    "\n",
    "                deg_graph = degree(gr, (graph_nodes.item()))\n",
    "                \n",
    "                \n",
    "                total_volume = deg_graph.sum()\n",
    "                recfield_volume = (mask*deg_graph).sum()\n",
    "                volume_range = recfield_volume/total_volume\n",
    "                total_vols[k] = total_volume\n",
    "                recfield_vols[k] = recfield_volume\n",
    "                \n",
    "                \n",
    "                #if receptive field volume is less than x% of total volume, resample\n",
    "                for iteration in range(max_iterations):  \n",
    "                    randInt = np.random.choice(range(graph_nodes), N, replace = replace)\n",
    "                    node_sample = torch.zeros(N*graph_nodes,device=device)\n",
    "                    offs  = torch.arange(N, device=device)*graph_nodes\n",
    "                    dirac_locations = (offs + torch.from_numpy(randInt).to(device))\n",
    "                    node_sample[dirac_locations] = 1\n",
    "                    \n",
    "                    mask = get_mask(node_sample, graph_edge_index, receptive_field).float()\n",
    "                    recfield_volume = (mask*deg_graph).sum()\n",
    "                    volume_range = recfield_volume/total_volume\n",
    "\n",
    "                    if volume_range > effective_volume_range:\n",
    "                        recfield_vols[k] = recfield_volume\n",
    "                        total_vols[k] = total_volume\n",
    "                        break;\n",
    "                \n",
    "                \n",
    "                dirac_locations2 = torch.from_numpy(randInt).to(device) + global_offset\n",
    "                global_offset += graph_nodes\n",
    "                \n",
    "                diracmatrix = torch.cat((diracmatrix, node_sample),0)\n",
    "                locationmatrix = torch.cat((locationmatrix, dirac_locations2),0)\n",
    "             \n",
    "                \n",
    "            \n",
    "                #for batch prime\n",
    "#                 dirac_indices = torch.arange(N, device=device).unsqueeze(-1).expand(-1, graph_nodes).contiguous().view(-1)\n",
    "#                 dirac_indices = dirac_indices.long()\n",
    "#                 dirac_indices += k*N\n",
    "#                 batch_prime = torch.cat((batch_prime, dirac_indices))\n",
    "\n",
    "\n",
    "\n",
    "            locationmatrix = diracmatrix.nonzero()\n",
    "\n",
    "\n",
    "            return Batch(batch = batch_index, x = diracmatrix, edge_index = original_edge_index,\n",
    "                         y = data.y, locations = locationmatrix, volume_range = volume_range, recfield_vol = recfield_vols, total_vol = total_vols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derandomize_clique_final(data, probabilities, draw=False, weight_factor = 0.35, clique_number_bounds = None ,fig = None, device = 'cpu'):\n",
    "       \n",
    "    row, col = data.edge_index\n",
    "    sets = probabilities.detach().unsqueeze(-1)\n",
    "    batch = data.batch\n",
    "    \n",
    "    no_loop_index,_ = remove_self_loops(data.edge_index)        \n",
    "    no_loop_row, no_loop_col = no_loop_index\n",
    "    num_graphs = batch.max().item() + 1\n",
    "    \n",
    "    \n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    for graph in range(num_graphs):\n",
    "       #  print(\"NEW GRAPH-----------------------------------\")\n",
    "            #select subset of nodes and edges corresponding to graph\n",
    "        mark_edges = batch[no_loop_row] == graph\n",
    "        nlr_graph, nlc_graph = no_loop_index[:,mark_edges]\n",
    "        nlr_graph = nlr_graph - total_index\n",
    "        nlc_graph = nlc_graph - total_index\n",
    "        batch_graph = (batch==graph)\n",
    "        graph_probs = sets[batch_graph].detach()\n",
    "        sorted_inds = torch.argsort(graph_probs.squeeze(-1), descending=True)\n",
    "        \n",
    "         #print(batch_graph)\n",
    "        \n",
    "        pairwise_prodsums = torch.zeros(1, device = device)\n",
    "        pairwise_prodsums = (torch.conv1d(graph_probs.unsqueeze(-1), graph_probs.unsqueeze(-1))).sum()/2\n",
    "        \n",
    "        self_sums = (graph_probs*graph_probs).sum()\n",
    "\n",
    "\n",
    "            \n",
    "        num_nodes = batch_graph.float().sum().item()\n",
    "        \n",
    "        current_set_cardinality = 0\n",
    "        \n",
    "        for node in range(int(num_nodes)):\n",
    "        #while current_set_cardinality <= 0:\n",
    "            #print(\"Current node: \", node)\n",
    "            ind_i = total_index + sorted_inds[node]\n",
    "            graph_probs_0 = sets[batch_graph].detach()\n",
    "            graph_probs_1 = sets[batch_graph].detach()\n",
    "            \n",
    "            graph_probs_0[sorted_inds[node]] = 0\n",
    "            graph_probs_1[sorted_inds[node]] = 1\n",
    "\n",
    "            pairwise_prodsums_0 = torch.zeros(1, device = device)\n",
    "            pairwise_prodsums_0 = (torch.conv1d(graph_probs_0.unsqueeze(-1),graph_probs_0.unsqueeze(-1))).sum()/2\n",
    "\n",
    "            self_sums_0 = (graph_probs_0*graph_probs_0).sum()\n",
    "\n",
    "            expected_weight_G_0 = (graph_probs_0[nlr_graph]*graph_probs_0[nlc_graph]).sum()/2\n",
    "            expected_clique_weight_0 = (pairwise_prodsums_0 - self_sums_0)\n",
    "            clique_dist_0 = weight_factor*0.5*(expected_clique_weight_0 - expected_weight_G_0)-expected_weight_G_0\n",
    "\n",
    "\n",
    "            pairwise_prodsums_1 = torch.zeros(1, device = device)\n",
    "            pairwise_prodsums_1 = (torch.conv1d(graph_probs_1.unsqueeze(-1),graph_probs_1.unsqueeze(-1))).sum()/2\n",
    "\n",
    "            self_sums_1 = (graph_probs_1*graph_probs_1).sum()\n",
    "\n",
    "            expected_weight_G_1 = (graph_probs_1[nlr_graph]*graph_probs_1[nlc_graph]).sum()/2\n",
    "            expected_clique_weight_1 = (pairwise_prodsums_1 - self_sums_1)\n",
    "            clique_dist_1 = weight_factor* 0.5*(expected_clique_weight_1 - expected_weight_G_1)-expected_weight_G_1\n",
    "\n",
    "\n",
    "#              #CALCULATE NUMBER OF EDGES AND CONFIRM IF CLIQUE\n",
    "#             print(\"clique_dist_0: \", clique_dist_0)\n",
    "#             print(\"clique_dist_1: \", clique_dist_1)\n",
    "\n",
    "            if clique_dist_0 >= clique_dist_1: \n",
    "                decided = (graph_probs_1==1).float()\n",
    "                current_set_cardinality = decided.sum().item()\n",
    "                current_set_max_edges = (current_set_cardinality*(current_set_cardinality-1))/2\n",
    "                current_set_edges = (decided[nlr_graph]*decided[nlc_graph]).sum()/2\n",
    "\n",
    "                if (current_set_edges != current_set_max_edges):\n",
    "                    sets[ind_i] = 0 #IF NOT A CLIQUE\n",
    "                else:\n",
    "                    sets[ind_i] = 1 #IF A CLIQUE\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                   sets[ind_i] = 0\n",
    "\n",
    "\n",
    "        if draw: \n",
    "            dirac = data.locations[graph].item() - total_index\n",
    "            if fig is None:\n",
    "                 f1 = plt.figure(graph,figsize=(16,9)) \n",
    "            else:\n",
    "                 f1 = fig\n",
    "            ax1 = f1.add_subplot(121)\n",
    "            g1,g2 = drawGraphFromData(data.to('cpu'), graph, vals=sets.squeeze(-1).detach().cpu(), dense=False,seed=dirac, nodecolor=True,edgecolor=False,seedhops=True,hoplabels=True,binarycut=False)\n",
    "            ax2 = f1.add_subplot(122)\n",
    "            g1,g2 = drawGraphFromData(data.to('cpu'), graph, vals=probabilities.detach().cpu(), dense=False,seed=dirac, nodecolor=True,edgecolor=False,seedhops=True,hoplabels=True,binarycut=False, clique = True)\n",
    "\n",
    "            clique_size = len(list(max_clique(g1)))\n",
    "#              print(\"NX Clique size is: \", clique_size)\n",
    "#              print(\"True clique number: \", graph_clique_number(g1))\n",
    "        \n",
    "        \n",
    "        total_index += num_nodes\n",
    "        \n",
    "\n",
    "    expected_weight_G = scatter_add(sets[no_loop_col]*sets[no_loop_row], batch[no_loop_row], 0, dim_size = num_graphs)\n",
    "    set_cardinality = scatter_add(sets, batch, 0 , dim_size = num_graphs)\n",
    "    return sets, expected_weight_G.detach(), set_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast derandomizer\n",
    "def derandomize_clique_final_speed(data, probabilities, draw=False, weight_factor = 0.35, clique_number_bounds = None ,fig = None, device = 'cpu', beam = 40):\n",
    "       \n",
    "    row, col = data.edge_index\n",
    "    sets = probabilities.detach().unsqueeze(-1)\n",
    "    blank_sets = torch.zeros_like(probabilities)\n",
    "    batch = data.batch\n",
    "    \n",
    "    no_loop_index,_ = remove_self_loops(data.edge_index)        \n",
    "    no_loop_row, no_loop_col = no_loop_index\n",
    "    num_graphs = batch.max().item() + 1\n",
    "    \n",
    "    max_cardinalities = torch.zeros(num_graphs)\n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    for graph in range(num_graphs):\n",
    "        #torch.cuda.memory_summary()\n",
    "        #print(\"NEW GRAPH-----------------------------------\")\n",
    "            #select subset of nodes and edges corresponding to graph\n",
    "        mark_edges = batch[no_loop_row] == graph\n",
    "        nlr_graph, nlc_graph = no_loop_index[:,mark_edges]\n",
    "        nlr_graph = nlr_graph - total_index\n",
    "        nlc_graph = nlc_graph - total_index\n",
    "        batch_graph = (batch==graph)\n",
    "        graph_probs = sets[batch_graph].detach()\n",
    "        sorted_inds = torch.argsort(graph_probs.squeeze(-1), descending=True)\n",
    "\n",
    "        \n",
    "         #print(batch_graph)\n",
    "#         pairwise_prodsums = torch.zeros(1, device = device)\n",
    "#         pairwise_prodsums = (torch.conv1d(graph_probs.unsqueeze(-1), graph_probs.unsqueeze(-1))).sum()/2\n",
    "#         self_sums = (graph_probs*graph_probs).sum()\n",
    "        num_nodes = batch_graph.long().sum()\n",
    "        #print(num_nodes)\n",
    "        \n",
    "        current_set_cardinality = 0\n",
    "        target_neighborhood = torch.tensor([])\n",
    "        #final_set = []\n",
    "        node = 0\n",
    "        max_width = beam\n",
    "        if num_nodes>max_width:\n",
    "            beam_width = max_width \n",
    "        else:\n",
    "            beam_width = num_nodes\n",
    "            \n",
    "        max_beam_weight = 0\n",
    "        max_weight_node = 0\n",
    "        graph_probs_1 = sets[batch_graph].detach()\n",
    "        max_cardinality = 0\n",
    "        \n",
    "        for node in range(beam_width):\n",
    "            blank_sets[batch_graph] = 0\n",
    "            current_set_cardinality = 0\n",
    "\n",
    "        #for node in range(int(num_nodes)):\n",
    "        #while current_set_cardinality <= 0:\n",
    "            #print(\"Current node here: \", current_set_cardinality)\n",
    "            ind_i = total_index + sorted_inds[node]\n",
    "\n",
    "            ind_i = total_index + sorted_inds[node]\n",
    "            blank_sets[ind_i] = 1\n",
    "            sets[ind_i] = 1 #IF A CLIQUE=\n",
    "            #final_set += [ind_i]\n",
    "            target_neighborhood = torch.unique(nlc_graph[nlr_graph == sorted_inds[node]])\n",
    "            decided = blank_sets[batch_graph]\n",
    "            #current_set_cardinality = decided.sum()\n",
    "            #print(\"current card: \", current_set_cardinality)\n",
    "            current_set_max_edges = (current_set_cardinality*(current_set_cardinality-1))/2\n",
    "            current_set_edges = (decided[nlr_graph]*decided[nlc_graph]).sum()/2\n",
    "            current_set_cardinality += 1\n",
    "\n",
    "            neighborhood_probs =  graph_probs[target_neighborhood]\n",
    "            #print(neighborhood_probs.shape)\n",
    "            neigh_inds = torch.argsort(neighborhood_probs.squeeze(-1), descending=True)\n",
    "            #if not neigh_inds:\n",
    "            sorted_target_neighborhood = target_neighborhood[neigh_inds]\n",
    "\n",
    "\n",
    "            for node_2 in sorted_target_neighborhood:\n",
    "            #    print(\"here\")\n",
    "                ind_i  = total_index + node_2\n",
    "                #graph_probs_0 = sets[batch_graph].detach()\n",
    "                #graph_probs_1 = sets[batch_graph].detach()\n",
    "\n",
    "    #             graph_probs_0[node] = 0\n",
    "    #             #graph_probs_1[node] = 1\n",
    "\n",
    "    #             pairwise_prodsums_0 = torch.zeros(1, device = device)\n",
    "\n",
    "                blank_sets[ind_i] = 1\n",
    "                sets[ind_i] = 1 #IF A CLIQUE\n",
    "                current_set_cardinality += 1\n",
    "                decided = blank_sets[batch_graph]\n",
    "                #current_set_cardinality = decided.sum()\n",
    "                current_set_max_edges = (current_set_cardinality*(current_set_cardinality-1))/2\n",
    "                current_set_edges = (decided[nlr_graph]*decided[nlc_graph]).sum()/2\n",
    "\n",
    "                if (current_set_edges != current_set_max_edges):\n",
    "    #                 print(\"current edges: \", current_set_edges)\n",
    "    #                 print(\"current max edges: \", current_set_max_edges)\n",
    "\n",
    "                    sets[ind_i] = 0 #IF NOT A CLIQUE\n",
    "                    blank_sets[ind_i] = 0  \n",
    "                    current_set_cardinality =  current_set_cardinality - 1\n",
    "\n",
    "            if current_set_cardinality > max_cardinality:\n",
    "                max_cardinality = current_set_cardinality\n",
    "            #blank_sets[ind_i] = 0\n",
    "\n",
    "        max_cardinalities[graph] = max_cardinality\n",
    "    #             else:\n",
    "    #                 sets[ind_i] = 1 #IF A CLIQUE\n",
    "    #                 blank_sets[ind_i] = 1\n",
    "    #                 #final_set += [ind_i]\n",
    "    #                 #target_neighborhood = torch.unique(nlc_graph[nlr_graph == ind_i])\n",
    "    #                 #print(target_neighborhood)\n",
    "\n",
    "    #             else:\n",
    "    #                    sets[ind_i] = 0\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "        if draw: \n",
    "            dirac = data.locations[graph].item() - total_index\n",
    "            if fig is None:\n",
    "                 f1 = plt.figure(graph,figsize=(16,9)) \n",
    "            else:\n",
    "                 f1 = fig\n",
    "            ax1 = f1.add_subplot(121)\n",
    "            g1,g2 = drawGraphFromData(data.to('cpu'), graph, vals=sets.squeeze(-1).detach().cpu(), dense=False,seed=dirac, nodecolor=True,edgecolor=False,seedhops=True,hoplabels=True,binarycut=False)\n",
    "            ax2 = f1.add_subplot(122)\n",
    "            g1,g2 = drawGraphFromData(data.to('cpu'), graph, vals=probabilities.detach().cpu(), dense=False,seed=dirac, nodecolor=True,edgecolor=False,seedhops=True,hoplabels=True,binarycut=False, clique = True)\n",
    "\n",
    "            clique_size = len(list(max_clique(g1)))\n",
    "#              print(\"NX Clique size is: \", clique_size)\n",
    "#              print(\"True clique number: \", graph_clique_number(g1))\n",
    "        \n",
    "        \n",
    "        total_index += num_nodes\n",
    "        \n",
    "\n",
    "    expected_weight_G = scatter_add(blank_sets[no_loop_col]*blank_sets[no_loop_row], batch[no_loop_row], 0, dim_size = num_graphs)\n",
    "    set_cardinality = scatter_add(blank_sets, batch, 0 , dim_size = num_graphs)\n",
    "    #print(set_cardinality)\n",
    "    return blank_sets, expected_weight_G.detach(), max_cardinalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Using license file /home/karalias/gurobi.lic\n",
      "Academic license - for non-commercial use only\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GET GUROBI GROUND TRUTHS\n",
    "from  cut_utils import solve_gurobi_maxclique\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "test_data_clique = []\n",
    "for data in testdata:\n",
    "    my_graph = to_networkx(Data(x=data.x, edge_index = data.edge_index)).to_undirected()\n",
    "    print(my_graph)\n",
    "    cliqno, _ = solve_gurobi_maxclique(my_graph, 500)\n",
    "    data.clique_number = cliqno\n",
    "    test_data_clique += [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data prep and fp:  0.19528794288635254\n",
      "Derandomization time:  0.6902110576629639\n",
      "Current batch:  1\n",
      "Time so far:  0.8925487995147705\n",
      "data prep and fp:  0.1503159999847412\n",
      "Derandomization time:  0.8023681640625\n",
      "Current batch:  2\n",
      "Time so far:  0.9594218730926514\n",
      "data prep and fp:  0.15389084815979004\n",
      "Derandomization time:  0.768791675567627\n",
      "Current batch:  3\n",
      "Time so far:  0.930201530456543\n",
      "data prep and fp:  0.1518864631652832\n",
      "Derandomization time:  0.978672981262207\n",
      "Current batch:  4\n",
      "Time so far:  1.1376447677612305\n",
      "data prep and fp:  0.15407228469848633\n",
      "Derandomization time:  0.7829701900482178\n",
      "Current batch:  5\n",
      "Time so far:  0.9444413185119629\n",
      "data prep and fp:  0.1525895595550537\n",
      "Derandomization time:  0.7348346710205078\n",
      "Current batch:  6\n",
      "Time so far:  0.8961632251739502\n",
      "data prep and fp:  0.03475189208984375\n",
      "Derandomization time:  0.06750249862670898\n",
      "Current batch:  7\n",
      "Time so far:  0.10378599166870117\n",
      "Average time per graph:  0.03027577546178078\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "#test_data = traindata\n",
    "test_data = testdata\n",
    "#shuffle(test_data)\n",
    "test_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
    "net.to(device)\n",
    "count = 1\n",
    "#Evaluation\n",
    "net.eval()\n",
    "\n",
    "gnn_nodes = []\n",
    "gnn_edges = []\n",
    "gnn_sets = {}\n",
    "max_samples = 1\n",
    "gnn_times = []\n",
    "num_samples = max_samples\n",
    "t_start = time.time()\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "\n",
    "    num_graphs = data.batch.max().item()+1\n",
    "    bestset = {}\n",
    "    bestedges = np.zeros((num_graphs))\n",
    "    maxset = np.zeros((num_graphs))\n",
    "\n",
    "    total_samples = []\n",
    "    for graph in range(num_graphs):\n",
    "        curr_inds = (data.batch==graph)\n",
    "        g_size = curr_inds.sum().item()\n",
    "        if max_samples <= g_size: \n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=False)\n",
    "        else:\n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=True)\n",
    "\n",
    "        total_samples +=[samples]\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    t_0 = time.time()\n",
    "    \n",
    "#     max_size = data.x.shape[0]\n",
    "#     #print(max_size)\n",
    "#     if (max_size <= max_samples):\n",
    "#             num_samples = max_size\n",
    "#     else:\n",
    "#             num_samples = max_samples\n",
    "#     sample_locations = np.random.choice(max_size,max_samples, replace=False)\n",
    "\n",
    "    for k in range(num_samples):\n",
    "        t_datanet_0 = time.time()\n",
    "        data_prime = get_diracs(data.to(device), 1, sparse = True, effective_volume_range=0.15, receptive_field = 7)\n",
    "  \n",
    "        initial_values = data_prime.x.detach()\n",
    "        data_prime.x = torch.zeros_like(data_prime.x)\n",
    "        g_offset = 0\n",
    "        for graph in range(num_graphs):\n",
    "            curr_inds = (data_prime.batch==graph)\n",
    "            g_size = curr_inds.sum().item()\n",
    "            graph_x = data_prime.x[curr_inds]\n",
    "            data_prime.x[total_samples[graph][k] + g_offset]=1.\n",
    "            g_offset += g_size\n",
    "            \n",
    "        retdz = net(data_prime)\n",
    "        \n",
    "        t_datanet_1 = time.time() - t_datanet_0\n",
    "        print(\"data prep and fp: \", t_datanet_1)\n",
    "        t_derand_0 = time.time()\n",
    "\n",
    "\n",
    "        sets, set_edges, set_cardinality = derandomize_clique_final_speed(data_prime,(retdz[\"output\"][0]), weight_factor =0.,draw=False, beam = 1)\n",
    "\n",
    "        t_derand_1 = time.time() - t_derand_0\n",
    "        print(\"Derandomization time: \", t_derand_1)\n",
    "\n",
    "\n",
    "        for j in range(num_graphs):\n",
    "            indices = (data.batch == j)\n",
    "            if (set_cardinality[j]>maxset[j]):\n",
    "                    maxset[j] = set_cardinality[j].item()\n",
    "                    bestset[str(j)] = sets[indices].cpu()\n",
    "                    bestedges[j] = set_edges[j].item()\n",
    "\n",
    "        #del retdz\n",
    "       # del data_prime\n",
    "    t_1 = time.time()-t_0\n",
    "    print(\"Current batch: \", count)\n",
    "    print(\"Time so far: \", time.time()-t_0)\n",
    "    gnn_sets[str(count)] = bestset\n",
    "    \n",
    "    gnn_nodes += [maxset]\n",
    "    gnn_edges += [bestedges]\n",
    "    gnn_times += [t_1]\n",
    "    #del sets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    count += 1\n",
    "    #if count > 200:\n",
    "        #break\n",
    "t_1 = time.time()\n",
    "total_time = t_1 - t_start\n",
    "print(\"Average time per graph: \", total_time/(len(test_data)))\n",
    "#print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLATTEN\n",
    "flat_list = [item for sublist in gnn_edges for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_edges = (flat_list)\n",
    "\n",
    "flat_list = [item for sublist in gnn_nodes for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_nodes = (flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean ratio: 0.8387401619438469 +/-  0.19620118617274399\n"
     ]
    }
   ],
   "source": [
    "tests = test_data_clique\n",
    "ratios = [gnn_nodes[i]/tests[i].clique_number for i in range(len(tests))]\n",
    "print(f\"Mean ratio: {(np.array(ratios)).mean()} +/-  {(np.array(ratios)).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('erdos': conda)",
   "display_name": "Python 3.7.7 64-bit ('erdos': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a03ac8f41426ce1099410c7b36e456bab34f9f50b0ea6affd1919e7da62ebae9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}