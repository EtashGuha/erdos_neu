{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First take care of all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from kernel.datasets import get_dataset\n",
    "import time\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from math import ceil\n",
    "from torch.nn import Linear\n",
    "from torch.distributions import categorical\n",
    "from torch.distributions import Bernoulli\n",
    "import torch.nn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pygraphviz as pgv\n",
    "from torch_geometric.utils import convert as cnv\n",
    "from torch_geometric.utils import sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "from torch.distributions import relaxed_categorical\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import GINConv, GATConv\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.utils import scatter_\n",
    "from torch_geometric.data import Batch \n",
    "from torch_scatter import scatter_min, scatter_max, scatter_add, scatter_mean\n",
    "from torch import autograd\n",
    "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\n",
    "from models import cut_MPNN\n",
    "from modules_and_utils import derandomize_cut, GATAConv,get_diracs, total_var\n",
    "import scipy\n",
    "import scipy.io\n",
    "import GPUtil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"facebook\", \"sf\",\"twitter\"]\n",
    "curr_dataset= datasets[1] \n",
    "#set random seed\n",
    "rseed = 201\n",
    "\n",
    "if curr_dataset==\"facebook\":\n",
    "    datasetname = \"facebook_graphs\"\n",
    "    dataset = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #set up facebook data \n",
    "    upper_limit = 15000\n",
    "    lower_limit = 0\n",
    "    path_to_facebook_dataset= \"%PATH\"\n",
    "    for file in os.listdir(path_to_facebook_dataset):\n",
    "            if file.endswith(\".mat\"):\n",
    "                adj_matrix = scipy.io.loadmat(path_to_facebook_dataset+str(file))\n",
    "                edge_index = from_scipy_sparse_matrix(adj_matrix['A'])[0]\n",
    "                x = torch.ones(adj_matrix['local_info'].shape[0])\n",
    "                if (adj_matrix['local_info'].shape[0] < lower_limit) or (adj_matrix['local_info'].shape[0] > upper_limit):\n",
    "                    continue\n",
    "                data_temp = Batch(x = x, edge_index = edge_index.long(), batch = torch.zeros_like(x).long())\n",
    "                data_proper = get_diracs(data_temp.to('cuda'), 1, sparse = True)\n",
    "                r,c = data_proper.edge_index\n",
    "                data = Batch(x = data_temp.x, edge_index = data_temp.edge_index)\n",
    "                degrees = degree(r, adj_matrix['local_info'].shape[0])\n",
    "                print(\"Graph specs: \")\n",
    "                print(\"number of nodes: \", adj_matrix['A'].shape[0])\n",
    "                print(\"average degree: \", degrees.mean(0))\n",
    "                print(\"total volume: \", data_proper.total_vol)\n",
    "                print(\"-------------\")\n",
    "                dataset += [data]\n",
    "\n",
    "elif curr_dataset==\"sf\":\n",
    "    datasetname = \"SF-295\"\n",
    "    dataset = get_dataset(datasetname,sparse=1)\n",
    "\n",
    "elif curr_dataset==\"twitter\":\n",
    "    path_to_twitter_dataset = \"%PATH\"\n",
    "    stored_dataset = open(path_to_twitter_dataset, 'rb')        \n",
    "    dataset = pickle.load(stored_dataset)    \n",
    "\n",
    "dataset_scale = 0.1\n",
    "total_samples = int(np.floor(len(dataset)*dataset_scale))\n",
    "dataset = dataset[:total_samples]\n",
    "\n",
    "num_trainpoints = int(np.floor(0.6*len(dataset)))\n",
    "num_valpoints = int(np.floor(num_trainpoints/3))\n",
    "num_testpoints = len(dataset) - (num_trainpoints + num_valpoints)\n",
    "\n",
    "\n",
    "traindata= dataset[0:num_trainpoints]\n",
    "valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\n",
    "\n",
    "testdata = dataset[num_trainpoints + num_valpoints:]\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#set up random seed \n",
    "torch.manual_seed(rseed)\n",
    "np.random.seed(2)   \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, recfield):\n",
    "    net.eval()\n",
    "    avg_loss = 0\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        data = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = recfield)\n",
    "        data = data.to(device)\n",
    "        retdict = net(data)\n",
    "        avg_loss += retdict['loss'][0].item()/len(data_loader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  0\n",
      "Current epoch:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7d14a9b589d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdata_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_diracs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_volume_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreceptive_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreceptive_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdata_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/lts2/karalias/repoz/erdos_neural/modules_and_utils.py\u001b[0m in \u001b[0;36mget_diracs\u001b[0;34m(data, N, n_diracs, sparse, flat, replace, receptive_field, effective_volume_range, max_iterations, complement)\u001b[0m\n\u001b[1;32m    263\u001b[0m                         \u001b[0mnode_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirac_locations\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreceptive_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                         \u001b[0mrecfield_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdeg_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0mvolume_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecfield_volume\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_volume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/lts2/karalias/repoz/erdos_neural/modules_and_utils.py\u001b[0m in \u001b[0;36mget_mask\u001b[0;34m(x, edge_index, hops)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/lts2/karalias/repoz/erdos_neural/modules_and_utils.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(x, edge_index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/karalias/anaconda3/envs/stalence/lib/python3.7/site-packages/torch_scatter/add.py\u001b[0m in \u001b[0;36mscatter_add\u001b[0;34m(src, index, dim, out, dim_size, fill_value)\u001b[0m\n\u001b[1;32m     70\u001b[0m                [2, 4, 4, 0, 0, 0]])\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/karalias/anaconda3/envs/stalence/lib/python3.7/site-packages/torch_scatter/utils/gen.py\u001b[0m in \u001b[0;36mgen\u001b[0;34m(src, index, dim, out, dim_size, fill_value)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdim_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_dim_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mout_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/karalias/anaconda3/envs/stalence/lib/python3.7/site-packages/torch_scatter/utils/gen.py\u001b[0m in \u001b[0;36mmaybe_dim_size\u001b[0;34m(index, dim_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=150\n",
    "numlayers=6\n",
    "elasticity = 0.25\n",
    "receptive_field= numlayers + 1\n",
    "val_losses = []\n",
    "\n",
    "#for sf/twitter\n",
    "#net =  cut_MPNN(dataset,numlayers, 64, 64,1, elasticity = elasticity)\n",
    "\n",
    "#for faceboook\n",
    "net =  cut_MPNN(dataset,6, 256, 24,1, elasticity = 0.25)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr_decay_step_size = 5\n",
    "lr_decay_factor = 0.95\n",
    "\n",
    "net.to(device).reset_parameters()\n",
    "optimizer = Adam(net.parameters(), lr=0.0001, weight_decay=0.00)\n",
    "net.train()\n",
    "retdict = {}\n",
    "for epoch in range(epochs):\n",
    "    print(\"Current epoch: \", epoch)\n",
    "    totalretdict = {}\n",
    "    count=0\n",
    "    #learning rate schedule\n",
    "    if epoch % lr_decay_step_size == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "   \n",
    "    net.train()\n",
    "    for data in train_loader:\n",
    "        count += 1 \n",
    "        optimizer.zero_grad(), \n",
    "        data = data.to(device)\n",
    "        data_prime = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = receptive_field)\n",
    "        data = data.to('cpu')\n",
    "        data_prime = data_prime.to(device)  \n",
    "        retdict = net(data_prime)\n",
    "        for key,val in retdict.items():\n",
    "            if \"sequence\" in val[1]:\n",
    "                if key in totalretdict:\n",
    "                    totalretdict[key][0] += val[0].item()\n",
    "                else:\n",
    "                    totalretdict[key] = [val[0].item(),val[1]]\n",
    "        \n",
    "        if epoch > 0:\n",
    "                retdict[\"loss\"][0].backward()\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(),1)\n",
    "                optimizer.step()                   \n",
    "\n",
    "        del data_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch count:  1\n",
      "Batch count:  2\n",
      "Batch count:  3\n",
      "Batch count:  4\n",
      "Batch count:  5\n",
      "Batch count:  6\n",
      "Batch count:  7\n",
      "Batch count:  8\n",
      "Batch count:  9\n",
      "Batch count:  10\n",
      "Batch count:  11\n",
      "Batch count:  12\n",
      "Batch count:  13\n",
      "Batch count:  14\n",
      "Batch count:  15\n",
      "Batch count:  16\n",
      "Batch count:  17\n",
      "Batch count:  18\n",
      "Batch count:  19\n",
      "Batch count:  20\n",
      "Batch count:  21\n",
      "Batch count:  22\n",
      "Batch count:  23\n",
      "Batch count:  24\n",
      "Batch count:  25\n",
      "Batch count:  26\n",
      "average time per graph: 0.347416846391285\n"
     ]
    }
   ],
   "source": [
    "#initialize\n",
    "net.eval()\n",
    "rcuts =  {}\n",
    "rvols =  {}\n",
    "rconds = {}\n",
    "mcuts =  []\n",
    "mvols =  []\n",
    "mconds = []\n",
    "best_cuts =  []\n",
    "best_vols =  []\n",
    "best_conds = []\n",
    "count = 0\n",
    "\n",
    "#select number of diracs\n",
    "num_diracs = 10\n",
    "\n",
    "cuts = torch.zeros((num_testpoints, num_diracs))\n",
    "conds =  torch.zeros((num_testpoints, num_diracs))\n",
    "vols =  torch.zeros((num_testpoints, num_diracs))\n",
    "randtargets = torch.zeros((num_testpoints, num_diracs))\n",
    "best_sets = {}\n",
    "totalvols = []\n",
    "\n",
    "\n",
    "\n",
    "t_0 = time.time()\n",
    "with torch.no_grad():\n",
    "    for data2 in test_loader:\n",
    "        batch = data2.batch\n",
    "        \n",
    "        count += 1\n",
    "           \n",
    "        print(\"Batch count: \", count)\n",
    "        dirac_count = 0 \n",
    "        for dirac in range(num_diracs):\n",
    "            data2 = data2.to(device)            \n",
    "            data_new = get_diracs(data2, 1, sparse=True, effective_volume_range=0.2)            \n",
    "            \n",
    "            feasible_vols = (data_new.recfield_vol/data_new.total_vol)*0.85\n",
    "            target_vol = torch.rand_like(feasible_vols, device=device)*feasible_vols + 0.1\n",
    "            data_new = data_new.to(device) \n",
    "            retdict2 = net(data_new, target_vol)\n",
    "            netprobs = retdict2['output'][0]\n",
    "            batch_new = data_new.batch\n",
    "            num_graphs = batch_new.max().item() + 1\n",
    "            e_i = data_new.edge_index \n",
    "            r,c = e_i\n",
    "            deg = degree(r)\n",
    "            bestcond = torch.ones(num_graphs)\n",
    "            bestcut = 1000*torch.ones(num_graphs)\n",
    "            bestvol = torch.zeros(num_graphs)\n",
    "            outp =  derandomize_cut(data_new.to('cuda'), netprobs.cuda(), target_vol.cuda()*data_new.total_vol.cuda(), elasticity=0.25, draw=False)\n",
    "            tv_hard = total_var(outp, data_new.edge_index.cuda(), data_new.batch.cuda())\n",
    "            vol_hard = scatter_add(deg*outp, batch_new, 0, dim_size = batch_new.max().item()+1)\n",
    "            mycond = tv_hard/vol_hard\n",
    "            conds[(count-1)*batch_size:count*batch_size, dirac] = mycond\n",
    "            cuts[(count-1)*batch_size:count*batch_size, dirac] = tv_hard\n",
    "            vols[(count-1)*batch_size:count*batch_size, dirac] = vol_hard\n",
    "            randtargets[(count-1)*batch_size:count*batch_size, dirac] = target_vol.cpu()*data_new.total_vol.cpu()          \n",
    "            dirac_count += 1\n",
    "t_final = time.time() - t_0\n",
    "print(f\"average time per graph: {t_final/len(testdata)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out mean conductance +/- std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanconds: 0.24204210937023163 +/- 0.006928241346031427\n"
     ]
    }
   ],
   "source": [
    "meanconds = conds.mean(0)\n",
    "print(f\"meanconds: {meanconds.mean()} +/- {meanconds.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4191, 0.1950, 0.4224,  ..., 0.5216, 0.5176, 0.1925],\n",
       "        [0.1867, 0.3671, 0.0612,  ..., 0.0769, 0.4304, 0.1595],\n",
       "        [0.0712, 0.1235, 0.0674,  ..., 0.4137, 0.5105, 0.2136],\n",
       "        ...,\n",
       "        [0.1273, 0.5107, 0.5413,  ..., 0.4979, 0.1599, 0.5830],\n",
       "        [0.4984, 0.0809, 0.2106,  ..., 0.2004, 0.3024, 0.2273],\n",
       "        [0.1200, 0.2984, 0.0147,  ..., 0.1250, 0.3855, 0.5035]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Stalence)",
   "language": "python",
   "name": "stalence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
