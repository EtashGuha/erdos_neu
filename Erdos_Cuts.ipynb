{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First take care of all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import time\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from math import ceil\n",
    "from torch.nn import Linear\n",
    "from torch.distributions import categorical\n",
    "from torch.distributions import Bernoulli\n",
    "import torch.nn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pygraphviz as pgv\n",
    "from torch_geometric.utils import convert as cnv\n",
    "from torch_geometric.utils import sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "from torch.distributions import relaxed_categorical\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import GINConv, GATConv\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.data import Batch \n",
    "from torch_scatter import scatter_min, scatter_max, scatter_add, scatter_mean\n",
    "from torch import autograd\n",
    "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\n",
    "from models import cut_MPNN\n",
    "from modules_and_utils import derandomize_cut, GATAConv,get_diracs, total_var\n",
    "import scipy\n",
    "import scipy.io\n",
    "import GPUtil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"facebook\", \"sf\",\"twitter\"]\n",
    "curr_dataset= datasets[1] \n",
    "#set random seed\n",
    "rseed = 201\n",
    "\n",
    "if curr_dataset==\"facebook\":\n",
    "    datasetname = \"facebook_graphs\"\n",
    "    dataset = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #set up facebook data \n",
    "    upper_limit = 15000\n",
    "    lower_limit = 0\n",
    "    path_to_facebook_dataset= \"%PATH\"\n",
    "    for file in os.listdir(path_to_facebook_dataset):\n",
    "            if file.endswith(\".mat\"):\n",
    "                adj_matrix = scipy.io.loadmat(path_to_facebook_dataset+str(file))\n",
    "                edge_index = from_scipy_sparse_matrix(adj_matrix['A'])[0]\n",
    "                x = torch.ones(adj_matrix['local_info'].shape[0])\n",
    "                if (adj_matrix['local_info'].shape[0] < lower_limit) or (adj_matrix['local_info'].shape[0] > upper_limit):\n",
    "                    continue\n",
    "                data_temp = Batch(x = x, edge_index = edge_index.long(), batch = torch.zeros_like(x).long())\n",
    "                data_proper = get_diracs(data_temp.to('cuda'), 1, sparse = True)\n",
    "                r,c = data_proper.edge_index\n",
    "                data = Batch(x = data_temp.x, edge_index = data_temp.edge_index)\n",
    "                degrees = degree(r, adj_matrix['local_info'].shape[0])\n",
    "                print(\"Graph specs: \")\n",
    "                print(\"number of nodes: \", adj_matrix['A'].shape[0])\n",
    "                print(\"average degree: \", degrees.mean(0))\n",
    "                print(\"total volume: \", data_proper.total_vol)\n",
    "                print(\"-------------\")\n",
    "                dataset += [data]\n",
    "\n",
    "elif curr_dataset==\"sf\":\n",
    "    datasetname = \"SF-295\"\n",
    "    dataset = TUDataset(root='/tmp/'+datasetname, name=datasetname)\n",
    "\n",
    "elif curr_dataset==\"twitter\":\n",
    "    path_to_twitter_dataset = \"%PATH\"\n",
    "    stored_dataset = open(path_to_twitter_dataset, 'rb')        \n",
    "    dataset = pickle.load(stored_dataset)   \n",
    "    dataset = [Data.from_dict(data) for data in dataset] \n",
    "\n",
    "\n",
    "dataset_scale = 0.1\n",
    "total_samples = int(np.floor(len(dataset)*dataset_scale))\n",
    "dataset = dataset[:total_samples]\n",
    "\n",
    "num_trainpoints = int(np.floor(0.6*len(dataset)))\n",
    "num_valpoints = int(np.floor(num_trainpoints/3))\n",
    "num_testpoints = len(dataset) - (num_trainpoints + num_valpoints)\n",
    "\n",
    "\n",
    "traindata= dataset[0:num_trainpoints]\n",
    "valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\n",
    "\n",
    "testdata = dataset[num_trainpoints + num_valpoints:]\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#set up random seed \n",
    "torch.manual_seed(rseed)\n",
    "np.random.seed(2)   \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, recfield):\n",
    "    net.eval()\n",
    "    avg_loss = 0\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        data = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = recfield)\n",
    "        data = data.to(device)\n",
    "        retdict = net(data)\n",
    "        avg_loss += retdict['loss'][0].item()/len(data_loader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  0\n",
      "torch.Size([1154])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/scratch/lts2/karalias/repoz/erdos_neural/modules_and_utils.py:278: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  locationmatrix = diracmatrix.nonzero()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/scratch/lts2/karalias/repoz/erdos_neural/models.py\u001b[0m(130)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    128 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    129 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 130 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    131 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    132 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "epochs=150\n",
    "numlayers=6\n",
    "elasticity = 0.25\n",
    "receptive_field= numlayers + 1\n",
    "val_losses = []\n",
    "\n",
    "#for sf/twitter\n",
    "#net =  cut_MPNN(dataset,numlayers, 64, 64,1, elasticity = elasticity)\n",
    "\n",
    "#for faceboook\n",
    "net =  cut_MPNN(dataset,6, 256, 24,1, elasticity = 0.25)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr_decay_step_size = 5\n",
    "lr_decay_factor = 0.95\n",
    "\n",
    "net.to(device).reset_parameters()\n",
    "optimizer = Adam(net.parameters(), lr=0.0001, weight_decay=0.00)\n",
    "net.train()\n",
    "retdict = {}\n",
    "for epoch in range(epochs):\n",
    "    print(\"Current epoch: \", epoch)\n",
    "    totalretdict = {}\n",
    "    count=0\n",
    "    #learning rate schedule\n",
    "    if epoch % lr_decay_step_size == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "   \n",
    "    net.train()\n",
    "    for data in train_loader:\n",
    "        count += 1 \n",
    "        optimizer.zero_grad(), \n",
    "        data = data.to(device)\n",
    "        data_prime = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = receptive_field)\n",
    "        data = data.to('cpu')\n",
    "        data_prime = data_prime.to(device)  \n",
    "        retdict = net(data_prime)\n",
    "        for key,val in retdict.items():\n",
    "            if \"sequence\" in val[1]:\n",
    "                if key in totalretdict:\n",
    "                    totalretdict[key][0] += val[0].item()\n",
    "                else:\n",
    "                    totalretdict[key] = [val[0].item(),val[1]]\n",
    "        \n",
    "        if epoch > 0:\n",
    "                retdict[\"loss\"][0].backward()\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(),1)\n",
    "                optimizer.step()                   \n",
    "\n",
    "        del data_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initialize\n",
    "net.eval()\n",
    "rcuts =  {}\n",
    "rvols =  {}\n",
    "rconds = {}\n",
    "mcuts =  []\n",
    "mvols =  []\n",
    "mconds = []\n",
    "best_cuts =  []\n",
    "best_vols =  []\n",
    "best_conds = []\n",
    "count = 0\n",
    "\n",
    "#select number of diracs\n",
    "num_diracs = 10\n",
    "\n",
    "cuts = torch.zeros((num_testpoints, num_diracs))\n",
    "conds =  torch.zeros((num_testpoints, num_diracs))\n",
    "vols =  torch.zeros((num_testpoints, num_diracs))\n",
    "randtargets = torch.zeros((num_testpoints, num_diracs))\n",
    "best_sets = {}\n",
    "totalvols = []\n",
    "\n",
    "\n",
    "\n",
    "t_0 = time.time()\n",
    "with torch.no_grad():\n",
    "    for data2 in test_loader:\n",
    "        batch = data2.batch\n",
    "        \n",
    "        count += 1\n",
    "           \n",
    "        print(\"Batch count: \", count)\n",
    "        dirac_count = 0 \n",
    "        for dirac in range(num_diracs):\n",
    "            data2 = data2.to(device)            \n",
    "            data_new = get_diracs(data2, 1, sparse=True, effective_volume_range=0.2)            \n",
    "            \n",
    "            feasible_vols = (data_new.recfield_vol/data_new.total_vol)*0.85\n",
    "            target_vol = torch.rand_like(feasible_vols, device=device)*feasible_vols + 0.1\n",
    "            data_new = data_new.to(device) \n",
    "            retdict2 = net(data_new, target_vol)\n",
    "            netprobs = retdict2['output'][0]\n",
    "            batch_new = data_new.batch\n",
    "            num_graphs = batch_new.max().item() + 1\n",
    "            e_i = data_new.edge_index \n",
    "            r,c = e_i\n",
    "            deg = degree(r)\n",
    "            bestcond = torch.ones(num_graphs)\n",
    "            bestcut = 1000*torch.ones(num_graphs)\n",
    "            bestvol = torch.zeros(num_graphs)\n",
    "            outp =  derandomize_cut(data_new.to('cuda'), netprobs.cuda(), target_vol.cuda()*data_new.total_vol.cuda(), elasticity=0.25, draw=False)\n",
    "            tv_hard = total_var(outp, data_new.edge_index.cuda(), data_new.batch.cuda())\n",
    "            vol_hard = scatter_add(deg*outp, batch_new, 0, dim_size = batch_new.max().item()+1)\n",
    "            mycond = tv_hard/vol_hard\n",
    "            conds[(count-1)*batch_size:count*batch_size, dirac] = mycond\n",
    "            cuts[(count-1)*batch_size:count*batch_size, dirac] = tv_hard\n",
    "            vols[(count-1)*batch_size:count*batch_size, dirac] = vol_hard\n",
    "            randtargets[(count-1)*batch_size:count*batch_size, dirac] = target_vol.cpu()*data_new.total_vol.cpu()          \n",
    "            dirac_count += 1\n",
    "t_final = time.time() - t_0\n",
    "print(f\"average time per graph: {t_final/len(testdata)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out mean conductance +/- std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanconds = conds.mean(0)\n",
    "print(f\"meanconds: {meanconds.mean()} +/- {meanconds.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:karalias-neuralll] *",
   "language": "python",
   "name": "conda-env-karalias-neuralll-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
